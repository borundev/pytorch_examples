{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GanLogic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWQX6OJn+SQQ6EM96hcrNn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "552dea4670bb4888a84f2448f35335e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5ce0b8ee17b54aabb610cb405207495f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0aa7602a710488cae91c6da74f043c6",
              "IPY_MODEL_7b6cb3f1773b44a5a580294820c9f1a9"
            ]
          }
        },
        "5ce0b8ee17b54aabb610cb405207495f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "c0aa7602a710488cae91c6da74f043c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_206adc26bdba42968560ece0f8965e6c",
            "_dom_classes": [],
            "description": "Epoch 0: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24fda9b0c3494218a3c7f45ba6d3f34e"
          }
        },
        "7b6cb3f1773b44a5a580294820c9f1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5137ce2ff42645bcbaf6ea4564ccbbcd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00, 10.53it/s, loss=0.743, v_num=0]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4de05b2980644306b09340a74c3ded8f"
          }
        },
        "206adc26bdba42968560ece0f8965e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24fda9b0c3494218a3c7f45ba6d3f34e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5137ce2ff42645bcbaf6ea4564ccbbcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4de05b2980644306b09340a74c3ded8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/borundev/pytorch_examples/blob/kaggle-dataset/GanLogic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4gibi6TvYKr"
      },
      "source": [
        "# Gan Logic\n",
        "\n",
        "This notebook contains my notes on gan logic without all the complexity that actually makes it work. So don't expect to get useful output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfeDEClXyMce",
        "outputId": "cd3cbc2a-1896-4958-96e1-5420b0b4ca87"
      },
      "source": [
        "!pip install -Uqqq pytorch_lightning\n",
        "!pip install -Uqqq wandb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 686kB 11.8MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829kB 35.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645kB 62.1MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 9.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 42.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296kB 39.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 53.8MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.9MB 11.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 8.7MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 49.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 8.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163kB 54.3MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 5.3MB/s \n",
            "\u001b[?25h  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orQ_HqrryVW6"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqdt44mWyXVZ"
      },
      "source": [
        "# A class to make hooks on modules and remove them when the destructor is called\n",
        "# I use this to diagnose when the forward and backward of a module are called\n",
        "\n",
        "class Hook():\n",
        "    def __init__(self,name,m):\n",
        "        self.name=name\n",
        "        self.m=m\n",
        "        self.ref_fw=m.register_forward_hook(self.forward_hook)\n",
        "        self.ref_bk=m.register_backward_hook(self.backward_hook)\n",
        "        \n",
        "\n",
        "    def remove(self):\n",
        "        if hasattr(self,'ref_fw'):\n",
        "           self.ref_fw.remove()\n",
        "           self.ref_bk.remove()\n",
        "\n",
        "    def forward_hook(self,*args,**kwargs):\n",
        "        print('forward called on',self.name)\n",
        "\n",
        "    def backward_hook(self,*args,**kwargs):\n",
        "        print('backward called on',self.name)\n",
        "\n",
        "    def __del__(self,*args,**kwargs):\n",
        "        self.remove()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXz6ynyfypYC"
      },
      "source": [
        "# Two functions to keep track of grads signifying if a backward call populated the grad tensor\n",
        "\n",
        "def gen_params():\n",
        "    r=torch.tensor([0])\n",
        "    try:\n",
        "        r=torch.tensor([torch.sum(p.grad != 0) for p in gen.parameters()]).sum()\n",
        "    except TypeError as e:\n",
        "        pass\n",
        "    return r\n",
        "\n",
        "def disc_params():\n",
        "    r=torch.tensor([0])\n",
        "    try:\n",
        "        r=torch.tensor([torch.sum(p.grad != 0) for p in disc.parameters()]).sum()\n",
        "    except TypeError as e:\n",
        "        pass\n",
        "    return r"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk2DtG5Hybqo"
      },
      "source": [
        "# Callback that has methods called right after backward call(s) and before calling zero grad\n",
        "\n",
        "class NewCallback(pl.callbacks.Callback):\n",
        "\n",
        "    def on_after_backward(self, trainer, pl_module):\n",
        "        print('after_backward generator',gen_params())\n",
        "        print('after_backward discriminator',disc_params())\n",
        "        print('-' * 20)\n",
        "\n",
        "    def on_before_zero_grad(self, trainer, pl_module, optimizer):\n",
        "        #print('before_zero_grad_generator', optimizer.name,gen_params())\n",
        "        #print('before_zero_grad_discriminator', optimizer.name,disc_params())\n",
        "        #print('*' * 20)\n",
        "        return\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjwBoyCoy2aw"
      },
      "source": [
        "class G(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W=torch.nn.Linear(1,1,bias=False)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.W(x)\n",
        "\n",
        "class D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W=torch.nn.Linear(1,1,bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self,x):\n",
        "        return self.sigmoid(self.W(x))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZRj9_U1zRw1"
      },
      "source": [
        "gen=G()\n",
        "disc=D()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61je5RppzT_2"
      },
      "source": [
        "h_gen=Hook('gen',gen.W)\n",
        "h_disc=Hook('disc',disc.W)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js4Qb0gwzVcz"
      },
      "source": [
        "X=torch.rand((10,1))\n",
        "y=torch.tensor(np.random.choice([0,1],10))\n",
        "data=DataLoader(list(zip(X,y)),batch_size=2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-EXgvbxzh4o"
      },
      "source": [
        "class Gan(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, gen, disc,\n",
        "                 latent_dim: int = 1,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.gen = gen\n",
        "        self.disc = disc\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            r = self.training_step_generator(batch)\n",
        "        elif optimizer_idx == 1:\n",
        "            r = self.training_step_discriminator(batch)\n",
        "        return r\n",
        "\n",
        "    def training_step_generator(self, batch):\n",
        "\n",
        "        xs, _ = batch\n",
        "        batch_size = xs.shape[0]\n",
        "\n",
        "        # note z.type_as(imgs) not only type_casts but also puts on the same device\n",
        "        z = torch.randn(batch_size, self.hparams.latent_dim, device=self.device)\n",
        "        generated_xs = self(z)\n",
        "        generated_y_score = self.disc(generated_xs)\n",
        "        generated_y = torch.ones(batch_size, 1, device=self.device)\n",
        "        g_loss = self.adversarial_loss(generated_y_score, generated_y)\n",
        "\n",
        "        self.log('generator/g_loss', g_loss, prog_bar=True)\n",
        "\n",
        "        return {'loss': g_loss}\n",
        "\n",
        "    def training_step_discriminator(self, batch):\n",
        "\n",
        "        xs, _ = batch\n",
        "        batch_size = xs.shape[0]\n",
        "\n",
        "        # note z.type_as(imgs) not only type_casts but also puts on the same device\n",
        "        z = torch.randn(batch_size, self.hparams.latent_dim, device=self.device)\n",
        "        generated_xs = self(z)\n",
        "        generated_y_score = self.disc(generated_xs)\n",
        "        generated_y = torch.zeros(batch_size, 1, device=self.device)\n",
        "        g_loss = self.adversarial_loss(generated_y_score, generated_y)\n",
        "\n",
        "        real_y_score = self.disc(xs)\n",
        "        real_y = torch.ones(batch_size, 1, device=self.device)\n",
        "        r_loss = self.adversarial_loss(real_y_score, real_y)\n",
        "\n",
        "        d_loss = (r_loss + g_loss) / 2.0\n",
        "\n",
        "        self.log('discriminator/d_loss', d_loss, prog_bar=True)\n",
        "\n",
        "        return {'loss': d_loss}\n",
        "\n",
        "    def adversarial_loss(self, y_hat, y):\n",
        "        return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt_g = torch.optim.Adam(self.gen.parameters())\n",
        "        opt_g.name = 'G'\n",
        "        opt_d = torch.optim.Adam(self.disc.parameters())\n",
        "        opt_d.name = 'D'\n",
        "        return [opt_g, opt_d], []\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qywNMyCx6wdn"
      },
      "source": [
        "The reason I got around to looking at this is because the way GANs work is that we train the generator and the discriminator one by one. Lets look at the generator first:\n",
        "\n",
        "1) We create some random noise and pass it through the generator. Thus the parameters of the generator are leaf nodes in the graph now. We then pass the result through the discriminator and thus the parameters of the discriminator are also leaf nodes in the graph now. Now when we call backward on the loss both these leaf nodes will get their grad tensors populated (unless we set `requires_grad=False`). At this step the optimizer for the generator will use the generator grads to update the weights and set those grads to zero for the next round. However, the discriminator grads are still set to non-zero values (unless it had `requires_grad` set to False).\n",
        "\n",
        "Now let's look at the discriminator. \n",
        "\n",
        "2) We create some random noise and pass it through the generator. This makes the paramaters of the generator leaf nodes of the graph. We then pass it through the discriminator making the discriminator's parameters also leaf nodes. Separately we pass real images through the discriminator making a separate computational graph with the discriminator's parameters as leaf nodes again. Now when we call backward on the losses of both the graphs, the grads of the parameters of both the generator and discriminator get populated. If we had not been careful of either seeting `requires_grad=False` for the discriminator in step (1) or to have reset the grads to zero then we will have the grads accumulate and the optimizer will end up using the wrong grads. The same goes for the grads for the generator computed in this step.\n",
        "\n",
        "We see this in action below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGqzvseL0PnN"
      },
      "source": [
        "batch=next(iter(data))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdeOa_qKzmzp"
      },
      "source": [
        "gan=Gan(gen,disc)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnLsHIYZ0gAF",
        "outputId": "648ab9ab-faae-43e5-9fa6-27d79de3421c"
      },
      "source": [
        "l=gan.training_step(batch,0,0)\n",
        "l['loss'].backward()\n",
        "gen_params(),disc_params()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "forward called on gen\n",
            "forward called on disc\n",
            "backward called on disc\n",
            "backward called on gen\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1), tensor(1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DVbFmaW_nDM"
      },
      "source": [
        "We see that there is a forward call on gen and disc followed by a backward call on disc and gen and both of them have their parameters' grads populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FeurhfA0-Wp"
      },
      "source": [
        "gan.zero_grad()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDwU3xoP1VvQ",
        "outputId": "2d4acbf4-a952-4956-8819-53fb321fd809"
      },
      "source": [
        "l=gan.training_step(batch,0,1)\n",
        "l['loss'].backward()\n",
        "gen_params(),disc_params()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "forward called on gen\n",
            "forward called on disc\n",
            "forward called on disc\n",
            "backward called on disc\n",
            "backward called on disc\n",
            "backward called on gen\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1), tensor(1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN0zDfsa_0XN"
      },
      "source": [
        "We see that there is a forward call on gen and two forward calls on disc followed by backward calls in reverse order.  Both of them have their parameters' grads populated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uij13nUF1c9S"
      },
      "source": [
        "gan.zero_grad()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSk_-5XS_l_b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsV-a-YFAD-o"
      },
      "source": [
        "Now we make a run of the pytorch_lighning trainer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "552dea4670bb4888a84f2448f35335e5",
            "5ce0b8ee17b54aabb610cb405207495f",
            "c0aa7602a710488cae91c6da74f043c6",
            "7b6cb3f1773b44a5a580294820c9f1a9",
            "206adc26bdba42968560ece0f8965e6c",
            "24fda9b0c3494218a3c7f45ba6d3f34e",
            "5137ce2ff42645bcbaf6ea4564ccbbcd",
            "4de05b2980644306b09340a74c3ded8f"
          ]
        },
        "id": "ISyGl2zsz10O",
        "outputId": "61d846c6-a491-4dd6-a3c0-457c541d76ed"
      },
      "source": [
        "trainer = pl.Trainer(gpus=0,\n",
        "                        max_epochs=1,\n",
        "                        limit_train_batches=1,\n",
        "                        limit_val_batches=0,\n",
        "                        progress_bar_refresh_rate=50,\n",
        "                        callbacks=[NewCallback()]\n",
        "                        )\n",
        "trainer.fit(gan,data)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: None, using: 0 TPU cores\n",
            "\n",
            "  | Name | Type | Params\n",
            "------------------------------\n",
            "0 | gen  | G    | 1     \n",
            "1 | disc | D    | 1     \n",
            "------------------------------\n",
            "2         Trainable params\n",
            "0         Non-trainable params\n",
            "2         Total params\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "552dea4670bb4888a84f2448f35335e5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "forward called on gen\n",
            "forward called on disc\n",
            "backward called on disc\n",
            "backward called on gen\n",
            "after_backward generator tensor(1)\n",
            "after_backward discriminator tensor(0)\n",
            "--------------------\n",
            "forward called on gen\n",
            "forward called on disc\n",
            "forward called on disc\n",
            "backward called on disc\n",
            "backward called on disc\n",
            "after_backward generator tensor(0)\n",
            "after_backward discriminator tensor(1)\n",
            "--------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUJiKGkMAOEW"
      },
      "source": [
        "We see that on the generator's `training_step` we have a forward call on gen and disc followed by backward calls in reverse order. However, only the generator's parameters have their grads populated.\n",
        "\n",
        "On the discriminator's `training_step` we have a forward call on gen and two on disc but only two backward calls on disc and none on gen. Only the discriminator's parameters have their grad tensor populated.\n",
        "\n",
        "The populating of the parameter's grads is being done in the correct way but looking at whoose backwards are being called is instructive. We reproduce it below manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0xADGpg9z--"
      },
      "source": [
        "gan.zero_grad()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48ANRgreA8s-"
      },
      "source": [
        "o1,o2 = gan.optimizers()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6XQiW24z46b",
        "outputId": "7409fcbf-e33a-4d49-ebf2-68106bc5f56a"
      },
      "source": [
        "o1.param_groups[0]['params'][0].requires_grad_(False)\n",
        "o2.param_groups[0]['params'][0].requires_grad_(False)\n",
        "o1.param_groups[0]['params'][0].requires_grad_(True)\n",
        "l=gan.training_step(batch,0,0)\n",
        "l['loss'].backward()\n",
        "print(gen_params(),disc_params())\n",
        "o1.zero_grad()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "forward called on gen\n",
            "forward called on disc\n",
            "backward called on disc\n",
            "backward called on gen\n",
            "tensor(1) tensor(0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajGnjBVu_eYD",
        "outputId": "16628628-2d37-4b89-ff3d-fa44aa3431d6"
      },
      "source": [
        "print(gen_params(),disc_params())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0) tensor(0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSIT7etT5FAH",
        "outputId": "3e97b55f-6d82-4229-ee23-66812d6f65b1"
      },
      "source": [
        "o1.param_groups[0]['params'][0].requires_grad_(False)\n",
        "o2.param_groups[0]['params'][0].requires_grad_(False)\n",
        "o2.param_groups[0]['params'][0].requires_grad_(True)\n",
        "l=gan.training_step(batch,0,1)\n",
        "l['loss'].backward()\n",
        "print(gen_params(),disc_params())\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "forward called on gen\n",
            "forward called on disc\n",
            "forward called on disc\n",
            "backward called on disc\n",
            "backward called on disc\n",
            "tensor(0) tensor(1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aklfNmRUBdPB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVIpXDqcBe_4"
      },
      "source": [
        "So we see that in the generator's forward and backward, while we set the discriminator's `requires_grad=False`, it still had a `backward` called on it and that is because the gradients needed to flow down to the generator's parameters.\n",
        "\n",
        "However, during the discriminator's forward and backward, the generator's parameters had the `requires_grad` set to False and didn't have backward called on it because it was not required in the backprop to compute anyone's gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0EYfcnt9htI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}